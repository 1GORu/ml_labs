## Churn.ipynb

> Решение задачи классификации оттока клиентов в рамках соревнования (метрика: **ROC-AUC**).

### Описание

Большинство участников достигали ROC-AUC ≈ 0.846 с `логистической регрессией`. После предобработки данных и отбора признаков мне удалось поднять результат до 0.853.

Многие использовали `CatBoost` с автоматическим подбором гиперпараметров и получали ROC-AUC ≈ 0.856. Я применил CatBoost, задав вручную ключевые, на мой взгляд, параметры — результат оказался практически сопоставим с лучшими публичными решениями.

Что касается самих `данных`, они, похоже, не обладают выраженной нелинейностью — поэтому линейная модель почти не уступает бустингу на деревьях.

Я отказался от `стекинга`, так как считаю его избыточным для большинства индустриальных задач: такие ансамбли сложно интерпретировать, поддерживать в продакшене и объяснять заинтересованным сторонам.

В лидерборде встречаются значения ROC-AUC до 0.99, достигнутые с помощью `полносвязных нейросетей`. Однако в открытом доступе циркулирует полный датасет, включая тестовую часть, что почти наверняка указывает на `data leak`.

---

## ADR_prediction.ipynb

Прогнозирование средней стоимости номера (ADR) в отелях центрального Чикаго на 6 месяцев вперёд с помощью линейной регрессии.

Данные: ежемесячная статистика по отелям Chicago Central Business District за 1994–2003 гг.  
Целевая переменная — Average Daily Rate (ADR), усреднённая по всем отелям и дням каждого месяца.

Несмотря на простоту, линейная регрессия достаточно хорошо улавливает общую динамику и сезонность временного ряда, хотя и не воспроизводит пики идеально.

**Вывод:** даже простая линейная модель может быть эффективна в задачах временных рядов, если данные содержат чёткую сезонность и тренд — без необходимости прибегать к сложным методам вроде нейросетей или ARIMA.

---
